# 5주차 스터디 정리
---
# Gradient Vanishing

- **문제**
  - 최적의 \( W \) 값을 찾기 위해 순전파와 역전파 과정을 반복함.
  - 시그모이드 함수 특성상 반복 계산 시 값이 점점 \( 0 \)에 가까워짐.

- **Sigmoid**
  - 기울기가 크거나 작아질수록 미분 시 \( 0 \)에 수렴.

- **Tanh**
  - 시그모이드의 개선 버전이나, \( 0 \) 범위를 제외하고 결국 \( 0 \)에 수렴.

- **해결책**
  - 다양한 Activation Function이 존재하지만, 완전한 기울기 소실 문제를 해결하지 못함.

---

# Optimization (경사 하강법 연장선)

## Momentum
- **개념**: 이전 단계의 속도를 함께 고려하여 진동을 줄이고 더 빠르게 학습.
- **공식**
  \[
  v_t = pv_{t-1} - \eta \nabla f(x_t)
  \]
  - \( v_t \): 현재 속도
  - \( p \): 관성값 (보통 \( 0.9 \))
  - \( \eta \): 학습률
  - \( \nabla f(x_t) \): 기울기

## Nesterov Momentum
- **개념**: 관성 방향으로 미리 이동한 뒤 기울기를 계산, 더 세밀한 조정 가능.
- **특징**: Momentum의 개선책.

## Adagrad (Adaptive Gradient Algorithm)
- **특징**
  - 각 기울기 제곱값의 누적 \( G_t \)에 따라 학습률을 점차 감소.
  - 오래 학습할수록 기울기가 \( 0 \)에 수렴.

## RMSProp (Root Mean Square Propagation)
- **개념**: Adagrad의 개선책으로, 최근 기울기에 더 큰 가중치를 부여.
- **특징**
  - \( \gamma \): 감쇠율 (보통 \( 0.9 \)).
  - 최근 기울기를 반영하여 학습률이 너무 빠르게 감소하지 않도록 작동.

## Adam (Adaptive Moment Estimation)
- **특징**
  - Momentum과 RMSProp을 결합한 방식.
  - 각 단계에서 학습률을 자동 조정.
  - 딥러닝에서 많이 사용됨.

---

# CNN (Convolutional Neural Network)

## 주요 개념
- **Conv 계층 연산**
  - 합성곱 연산으로 특징 추출.
- **Zero Padding**
  - 입력 데이터 외곽에 \( 0 \)을 추가하여 크기 유지.
- **Dropout**
  - 일부 뉴런을 랜덤하게 비활성화하여 과적합 방지.
