## 풀이 방법:
1. 인공 신경망의 최종 출력값을 구한다. (y값)
2. MSE를 통해 오차를 계산한다.
3. 전체 오차를 구한다. (출력이 하나이므로 생략.)
4. 역행!! (오차함수를 가중치로 미분한 값 * 학습률)을 구한다.
5. 위에서 구한 값을 원본 가중치에서 뺄셈.
6. 4~5 단계를 가중치마다 반복.

### 필요한 함수: sigmoid
```python
import numpy as np
# z = 가중치 합.
# z에서 h 또는 o로 변환하기 위한 시그모이드 함수를 구현하는 과정.
def sigmoid(z):
    return 1 / (1 + np.exp(-z))
```

### MSE 식?
실제값과 예측값(출력값)이 필요.
```markdown
$$
E_{o1} = \frac{1}{2} ( \text{target}_{o1} - \text{output}_{o1} )^2
$$
```

### 오차함수를 가중치로 미분한 값?
```markdown
$$
\frac{\partial L}{\partial w} = \frac{\partial L}{\partial \hat{y}} \cdot \frac{\partial \hat{y}}{\partial z} \cdot \frac{\partial z}{\partial w}
$$
```

### 문제의 값들
- x1: 0.1
- x2: 0.2
- w1: 0.15
- w2: 0.2
- w3: 0.25
- w4: 0.3
- w5: 0.4
- w6: 0.45
- 학습률: 0.5
- 실제값: 0.3

## 풀이 시작
- sigmoid 함수는 solve.ipynb, h값 연산은 계산기를 사용했습니다............
1. y값 구하기
    - w1x1 + w2x2 = z1
        - (0.15 * 0.1) + (0.2 * 0.2)
          = 0.015 + 0.04
          = 0.055
    - sigmoid(z1) = h1
        - 0.513746534902355
    - w3x1 + w4x2 = z2
        - (0.25 * 0.1) + (0.3 * 0.2)
          = 0.025 + 0.06
          = 0.085
    - sigmoid(z2) = h2
        - 0.5212372149662741
    - w5h1 + w6h2 = y
        - (0.4 * 0.5137...) + (0.45 * 0.5212...)
          = 0.205498613960942

2. MSE 통해 오차를 계산
    - 오차 = (0.5)(실제값-y값)^2
        - {(0.3 - 0.2054...)^2} / 2
          = {(0.094501386039058)^2} / 2
          = (0.0089305119633031) / 2
          = 0.0044652559816515

3. 전체 오차 구하기
    - 출력값 y가 하나뿐이므로 생략

4. 역행. 미분*학습률
    - (전체오차 미분) × (시그모이드의 미분) × (역행한 자리의 h값 또는 x값)
      = $ {- ( \text{target}_{o1} - \text{output}_{o1} )} \times {o_{1} \times (1 - o_{1})} \times {\text{h}} $
        - w5 -> - (0.3 - 0.2054...) * (0.2054...(1 - 0.2054...)) * (0.5137...)
          = - 0.094501386039058 * 0.1632689336210737 * 0.513746534902355
          = -0.007926667480886
        - w6 -> - (0.3 - 0.2054...) * (0.2054...(1 - 0.2054...)) * (0.5212...)
          = - 0.094501386039058 * 0.1632689336210737 * 0.5212372149662741
          = -0.0080422422362148
        - w1 -> - (0.3 - 0.055) * (0.055(1 - 0.055)) * (0.1)
          = - 0.245 * 0.051975 * 0.1
          = -0.0012733875
        - w2 -> - (0.3 - 0.055) * (0.055(1 - 0.055)) * (0.2)
          = - 0.245 * 0.051975 * 0.2
          = -0.002546775
        - w3 -> - (0.3 - 0.085) * (0.085(1 - 0.085)) * (0.1)
          = - 0.215 * 0.077775 * 0.1
          = -0.0016721625
        - w4 -> - (0.3 - 0.085) * (0.085(1 - 0.085)) * (0.2)
          = - 0.215 * 0.077775 * 0.2
          = -0.003344325

5. 가중치 업데이트
    - 원본 - (학습률)(4번의결과값)
        - w1+ = w1 - (0.5)(-0.0012733875)
          = 0.15 - (-0.000636693795)
          = 0.150636693795
        - w2+ = w2 - (0.5)(-0.002546775)
          = 0.2 - (-0.0012733875)
          = 0.2012733875
        - w3+ = w3 - (0.5)(-0.0016721625)
          = 0.25 - (-0.00083608125)
          = 0.25083608125
        - w4+ = w4 - (0.5)(-0.003344325)
          = 0.3 - (-0.0016721625)
          = 0.3016721625
        - w5+ = w5 - (0.5)(-0.007926667480886)
          = 0.4 - (-0.003963333740443)
          = 0.403963333740443
        - w6+ = w6 - (0.5)(-0.0080422422362148)
          = 0.45 - (-0.0040211211181074)
          = 0.4540211211181074
        
6. 업데이트 완료
    - 모든 가중치를 업데이트했다!