5주차 강의 내용 정리
===================

## Gradient Vanishing

- 최적의 W값을 찾기 위해 (W는 가중치)
  순전파, 역전파 과정을 계속 거친다.
- 시그모이드 함수 성질 때문에
  계산 과정을 반복하면 점점 0에 가까워지는 값을 출력한다.

# sigmoid

- 기울기가 크거나 작아짐에 따라 미분 시 0이 되는 형태이다.

# tanh 

- sigmoid에 개선 버전이지만 0 범위를 제외하면 결국 0에 수렴한다.

# Activation Funcions 

- sigmoid, tanh 함수 외에도 
  ReLU, Leaky ReLU, Maxout, ELU 같은 여러 개선 함수들이 존재한다.

- 하지만 소실 문제는 해결하지 못한다.

## Optimization(경사 하강법 연장선) 

# Momentum

- 사전적 정의 : 외부에서 힘을 받지 않는 한 정지해 있거나
               운동 상태를 지속하려는 성질

- 이전에 이동했던 방향을 기억하면서 
  이전 기울기의 크기를 고려하여 어느정도 추가로 이동

# Nesterov Momentum

- 좀 더 세밀 조정 가능, 기존 Momentum의 개선책이다.

- 모멘텀 값이 적용된 지점에서 기울기 값을 계산.

- 모멘텀으로 절반 정도 이동한 후 어떤 방식으로 이동할지 다시 계산 후
  스텝을 결정하여 단점 극복

# Adagrad(Adaptive Gradient Algorithm)


- 손실 함수 곡면의 변화에 따라 적응적으로 학습률을 정하는 알고리즘

- 고정된 학습률이 아닌, 변화하는 학습률이다.

- 결국 가중치 변화는 점차적으로 줄어들고
  오래 학습할 수록 기울기는 0에 수렴한다.

- 많이 변화한 변수가 최적해에 근접했을 거란 가정 하에 작은 크기로 이동하면서
  세밀하게 값을 조정하고 반대로 적게 변화한 변수들은 학습률을 크게 하여,
  오차 값을 줄이고자 하는 방법

# RMSProp

- AdGrad에서 학습이 안되는 문제 해결 위해 하이퍼 파라미터 추가

- 변화량이 더 클수록 학습률은 작아짐.

- 이로 인한 조기 종료 문제 해결 위해 
  학습률 크기를 비율로 조정할 수 있도록 제안된 방법

- 최단 경로의 곡면 변화량 측정을 위한 지수 가중 이동 평균 사용

# Adam

- Momentum + RMSProp = Adam

- 진행하던 속도에 관성을 주고, 
  최근 경로의 곡면의 변화량에 따른 적응적 학습률을 갖는 알고리즘.

## CNN (Convolution Neural Network, 합성곱 신경망)

- 이미지 데이터를 학습하고 인식하는데 특화된 알고리즘

- Convolution : 이미지 프로세싱에서 일정한 패턴으로 변환하기 위해 수행하는 행렬연산.

# Zero Padding

- image 주위를 0으로 둘러주는 과정

# Pooling

- Convolution을 거쳐서 나온 activation maps이 있을 때 
  이를 이루는 convolution layer을 resizing하여 새로운 layer를 얻는 것

- 목적 
   - input size를 줄인다.
   - overfitting을 조절한다.
   - 특징을 잘 뽑아낸다.
   - 지역적 이동에 노이즈를 줌으로써 일반화 성능을 올려준다.

- Max Pooling : 정해진 크기 안에서 가장 큰 값만 뽑아낸다.

- Average Pooling : 정해진 크기 안의 값들의 평균을 뽑아낸다.


# Dropout

- 과적합을 방지하기 위해 사용된다. 
  네트워크 일부를 생략하는 것.

- 학습 사이클이 진행되는 동안 무작위로 일부 뉴런 생략하여 Overfitting을 방지.