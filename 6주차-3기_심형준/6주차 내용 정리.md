시계열 데이터와 순환 신경망
=========================

## 시계열 데이터

- 시간의 흐름에 따라 순차적으로 측정된 데이터
 - 일정 시간 간격 배치 데이터들의 수열
 - 시간이 중요 변수로 작용 -> 기준?
 - 시간 흐름에 따른 변화 관찰
 - 무작위 섞을 시 -> 의미 없는 데이터

# RNN(Recurrent Neural Network)

- 임의의 길이를 가진 시퀀스 데이터나 시계열 데이터 분석이 가능한 신경망
 - 문장, 문서, 오디오 샘플 등을 입력 받을 수 있다.
 - 자동 번역, 스피치 투 텍스트 등 자연어 처리에 유용

- 순환 뉴런
 - 입력을 받아 출력을 반들고 자신에게도 출력을 보내는 뉴런
 - 각 타임스텝마다 입력과 이전 타임스텝의 출력을 입력으로 받음

- 순환 층
 - 모든 뉴런이 타임 스텝 t마다 입력 벡터와 이전 타임스텝의 출력을 입력으로 받음
 - 입력과 출력이 모두 벡터
 - 입력을 위한 가중치와 이전 타임스텝 출력을 위한 가중치

 # BPTT
 - 타임스텝으로 네트워크를 펼치고 역전파 사용하여 업데이트
 - 비용 함수의 그레이디언트를 펼쳐진 네트워크를 따라 역방향으로 전파
 - 비용 함수를 계산한 모든 출력에서 역방향으로 전파
 - 각 타임 스텝마다 같은 매개변수 사용되기 때문에 모든 타임 스텝에 걸쳐 합산

 # LSTM
 - RNN의 문제점 
  - 장기 기억의 부재
  - 학습속도 저하 문제(많은 연산량)
  - gradient 손실

 - 위 단점 극복을 위해 나온 모델
  - Forget gate : 기억할 정보 선별
  - Cell state : 장기 상태라고 불림
 
 - RNN보다 장기기억력이 뛰어나고, Gradient 소실 문제 개선

  # Vector to Sequence Network
  - 입력을 Vector로 받아 Sequence를 출력하는 모델
  - 이미지 입력 받은 후 그에 대한 서술 출력
  - 단일 단어 입력받아 그에 대한 서술 출력 
 
  # Sequence to Sequence Network
  - 입력과 출력 모두 Sequence인 네트워크
  - 주식 데이터 같은 시계열 데이터 예측 유용
  - 일반적인 네트워크 형태로는 입력과 출력 사이즈가 달라지는 문제 대응 힘듬

# Encoder-Decoder 구조

- NLP에서 Sequence to Sequence는 대부분 이 구조
1. encoder에서 입력을 받는다. 
2. Context Vector로 만든다.
3. Decoder에서 Context Vector를 입력받아 출력을 내보낸다.

- EX: 번역기, 텍스트 요약

## 자연어처리

# 어려움
- 숫자로 의미를 함축하는 것이 어렵기 때문이다.

# 토큰화
- 정량화 전, 자연어의 천문학적 경우의 수를 의미를 가진 단위로 나눈 뒤, 변환 가능 형태로 만들어준다.

- 형태소 별로 나누는 방법이 일반적.

- 생성형 AI 서비스에서 TOKEN의 의미

# Word Embedding
- 자연어 전처리에서 가장 어렵고 중요한 작업

- 각 토큰을 의미 있는 Vectro로 변환

- 초기에는 원-핫 인코딩 사용

- 문장 내 분포와 동시 언급 등을 이용한 알고리즘

- 최근에는 LLM을 사용하는 방법론이 연구되기 도함

- 매우 고차원의 데이터

# EN/DE-Coder 구조 문제
- 개선되었지만 여전한 Gradient Vanishing

- Context Vector에 모든 정보 압축으로 인해 정보가 손실

- 입력이 조금만 길어도 막대한 양의 정보 손실 발생

# Attention Mechanism
- seq2seq의 구조적인 문제 개선을 위해 연구된 알고리즘

- 디코더에서 출력 단어를 예측하는 시점마다,  
  인코더에서 전체 입력 문장 다시 한번 참고

- 전체 입력 문장을 동일 비율 참고 X,  
  해당 시점에서 예측해야 할 단어와 연관이 있는 부분 더 집중

# Transformer
- Attention mechanism 만으로 구성한 딥러닝 모델

- 현재 모든 LLM, 나아가 Multi Modal Model의 중추가 됨  

- 같은 파라미터 수 대비 RNN 보다 빠른 속도  

