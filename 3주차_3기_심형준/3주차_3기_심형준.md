Gradient Descent - 경사하강법 

함수 f(x)의 최솟값 = "f'(x) = 0"이 되는 지점
이 아이디어로 출발해보자.

goal of Machine Learning -> minimize Loss Function
Loss function = 0이 되는 지점

전체 Lossf(x)의 형태는 알 수 없다. 기울기는 계산 가능

경사하강법 수행 시 필요한 하이퍼 파라미터
- Learning Rate(학습률), Num of Epochs
- Learning Rate : 가중치 업데이트에 gradient를 얼마나 반영할 것인가?
- Num of Epochs : 업데이트를 얼마나 반복할 것인가?

경사하강법의 문제
- Learning rate 의 적절한 값에 대한 의문
- data scale 문제
- 손실 함수의 도함수가 0이 되는 지점이 여러 개
- 학습 과정에 거치는 모든 함수가 미분 가능이어야 함

  global minimum & local minimum
  - global minimum을 정확히 계산할 수 있는 공식/알고리즘은 없다
  - local minimum을 피하기 위한 방법으로 momentum, Adagrad 등
    가중치 정규화, 앙상블 기법이 있다..

  Feature Scaling의 수렴에 있어 중요.
  - 같은 scale을 가진 Feature들이면 등치선이 고르게 분포
  - 모든 Feauture은 동일 중요도 다룬다고 표현 가능

  적절한 learning rate 설정의 중요성
  - 너무 작으면 수렴 속도 느림
  - 너무 크면 최적점을 지나쳐 수렴 못할 가능성 존재
  - learning rate를 변경하며 학습 반복 
    또는 learning rate scheduling 사용

경사하강법의 적용
- 다양한 문제 적용 가능한 알고리즘은
- Class 확률값 예측 분류 문제 사용 가능
  -> Cross Entropy Loss 미분 가능
- MLP와 함께 딥러닝 기반 알고리즘으로, 
  모델 가중치 학습법으로 기능

비지도학습 정의
- 스스로 학습, 숨겨진 구조 찾도록.

군집화(Clustering)
- 데이터 포인트의 특징, 분포 파악 후 n개의 군집으로 묶기
- 데이터 포인트 간의 거리, 유사도 등과 같은 
  데이터에서 추출가능 특성 사용하여 수행
- 수행 알고리즘 예 : K-means, Mean-Shift, DBSCAN 등

특이치 탐지
- 일반적인 데이터 포인트들과 다른 
  특이 데이터 포인트 검출 알고리즘(from. 데이터셋)
- Isolation Forest

생성형 작업
- 이미지, 텍스트 등의 생성 작업 = 비지도학습
- 정답 X, 훈련 데이터 세트의 분포 학습
- GAN, VAE 등의 생성형 모델이 이에 해당

GPT4, DALLE 등의 generative model = 비지도 학습?
- GPT, BERT 같은 LLM의 경우 대규모 말뭉치 통해 학습
  이 데이터에 정답 따로 존재X
- 학습 과정에서 대부분 지도학습 방법론 사용하기 때문에
  자기 지도 학습 (Self-supervised Learning) 용어 사용
- DaLLE나 Stable Diffusion 같은 이미지 생성 모델도 마찬가지.

차원 축소 알고리즘
- 고차원 데이터에 데이터 특성 반영, 저차원에 투영 알고리즘
- 고차원 데이터가 가진 "차원의 저주" 일부 해결, 효율적 학습을 만듬
- 데이터가 가진 특성 or 정보 손실 가능성이 있기에 충분한 고려 후 사용

DBSCAN
(Density-based spatial clustering of applications with noise)
- 밀도 기반 클러스터링 방법론
- 단순 명료 군집화 방식 사용, 복잡한 형태 군집도 구분 가능
  
  Core Point
  - 임의의 DataPoint Epsilon 반경 안, minPts 이상 Point 존재 시
    Core Point
  - Epsilon : Date Point 반경 지정
  - Noise : Core Point의 Epsilon 반경 외의 데이터

Isolation Forest
- Decision Tree 변용, 
  모든 데이터 포인트 고립시킬 때, 빨리 고립되는 포인트가 특이치
  -> 기반 아이디어
- 모든 포인트 고립될 때까지 Tree 생성
- Root와 가까운 Leaf 위치 데이터 포인트가 특이치일 것

- Large Data Set에 대해서 잘 작동
- 시각적 확인 힘든 고차원 데이터 특이치 효과적 검출 가능
- 연관성 X, 선형적 X Feature 구성 데이터가 잘 작동하여
  실업에서 많은 사용

준지도 학습
- 지도학습 필요 데이터 만들기 어려움
- 작은 숫자 라벨링 데이터 + 큰 숫자 언라벨링 데이터 이용
  그리고 지도 학습 Task 수행
- 비슷한 데이터 -> 비슷한 라벨 가짐
  => 결정경계 만드는데 도움
- 사용량 : Classification > Regression 
- 일반적으로, Classificaiton Labeling 작업이 더 비용과 시간 많이 듬
- 준지도학습 방법론은 대부분 분류 작업 전제

준지도학습 가정
- Smoothness Assumption 
  두 데이터 포인트가 가깝다 -> 그에 해당 출력도 가까울 것
- Cluster Assumption
  두 데이터 포인트가 같은 군집에 속한다 -> 같은 class일 것

준지도 학습 방법론
- Proxy Label Method
- Consistency training
  
  Proxy Label Method
  - Labeled Date로 미리 학습된 모델 이용하여 
    Unlebled Data를 분류
  - Labeled Date + Unlebled Data 
    -> Labeling한 Pseudo-Labeled Date 함께 사용하여
       모델 재훈련
  - Labeled Data로 학습된 모델도 추정함수이기에
    Labeling 정확성 보장 x
    -Pseudo-Label의 신뢰성 높은 데이터만 사용
    -2번 과정 순차적 다수 반복

   Consistency training
   - Data에 작은 변화를 주어도 예측 결과 같을 것
   - Unlebled Data를 증강하여 
     원본 같은 Label 나오도록 추가 학습 진행

    