## 순환 신경망과 NLP
---

### 1. 시계열 데이터와 순환신경망
#### 시계열 데이터
- 시간 순서에 따라 측정된 데이터
    - 예: 주가, 날씨 변화 등
- 시간 순서가 중요, 섞이면 의미가 없는 데이터

#### RNN (Recurrent Neural Network)
- 순환 구조를 가진 신경망으로, 시계열 데이터나 순차적 데이터를 분석 가능
- 각 타임스텝마다 입력과 이전 타임스텝의 출력을 입력 받음
- BPTT (Back Propagation Through Time)
    - 타임스텝으로 네트워크를 펼치고 역전파를 사용해 업데이트
- **한계**:
  - 장기 기억 부재
  - 학습 속도 저하 및 Gradient 소실 문제

#### LSTM (Long Short-Term Memory)
- RNN의 한계를 극복한 구조.
- Forget Gate와 Cell State를 사용해 장기 기억력 강화
- Gradient 소실 문제를 개선

#### seq2vec
- sequnce 입력 > vector 출력
    - ex: 문장입력 > 감정 분류

#### vec2seq
- vector 입력 > seqence 출력
    - ex: 이미지 입력 > 문장 서술

#### seq2seq
- seqence 입력 > seqence 출력
    - ex: 시계열 데이터 > 시계열 데이터 (주식)

#### Encoder-Decoder 구조
- Encoder seq2vec
- Decoder vec2seq
- ex: 번역기, 텍스트 요약

---
### 2. 자연어 처리와 LLM
#### 자연어 처리의 특징
- 데이터를 숫자로 정량화해야 처리 가능
- 기존 변환 방식은 단순한 규칙에 기반(의미를 모름)

#### Tokenization
- 의미단위로 나눔

#### Word Embedding
- 단어를 벡터로 변환하여 의미를 함축
- 초기: 원-핫 인코딩
- 이후: 문장 내 분포와 동시 언급 등을 이용한 알고리즘 (Word2Vec, CBOW)

#### encoder-decoder 구조의 한계
- gradient vanishing 
- context vector로 모든 정보를 압축하니 정보손실

#### Attention Mechanism
- 출력 단어를 예측하는 시점마다, 인코더에서의 전체 입력 문장을 참고
- 목적: 입력 데이터의 중요한 부분에 집중.
- Q (Query): 디코더의 현재 상태 (t시점 출력하는 상태)
- K (Keys): 인코더의 은닉 상태
- V (Values): 인코더의 은닉 상태 값

- Query와 Key의 유사도를 계산해 중요도를 반영해서 V에 활용

#### Transformer
- Attention Mechanism만으로 구성한 딥러닝 모델
- Self-Attention Mechanism(Q, V, K 모두 같은 입력 문장 활용)을 활용
- RNN 보다 빠름
- 모든 입력 단어 간의 관계를 학습

#### Transformer 기반 LLM
- 다양한 작업에 뛰어난 성능 제공
- 이미지, 오디오 등 다중 모달 학습 가능
- 활용
    - Prompt Tuning, Fine Tuning, Workflow
- 한계
    - 학습 및 튜닝에 고비용
    - 데이터 편향
    - 해석 불가능
