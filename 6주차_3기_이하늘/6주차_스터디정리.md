# 순환 신경망과 NLP 요약

## 목차
1. 시계열 데이터와 순환신경망
2. 자연어 처리와 LLM
3. 어텐션 알고리즘과 트랜스포머

---

## 1. 시계열 데이터와 순환신경망
### 시계열 데이터
- 시간의 흐름에 따라 순차적으로 측정된 데이터.
- 시간 순서가 중요한 변수로 작용.

### RNN (Recurrent Neural Network)
- 시계열 데이터와 문장 등 순차 데이터 처리에 적합.
- **구조**: 순환 뉴런은 이전 타임스텝의 출력을 현재 입력과 함께 처리.
- **문제점**: 장기 기억 부족, 학습 속도 저하, 기울기 소실 문제.

### LSTM (Long Short-Term Memory)
- RNN의 단점을 보완하기 위해 개발.
- **구성**: Forget Gate와 Cell State를 통해 장기 기억력 향상, 기울기 소실 문제 개선.

---

## 2. 자연어 처리와 LLM (Large Language Model)
### 자연어 처리의 어려움
- 자연어를 숫자로 정량화하며 의미를 포함하도록 변환.
- 토큰화(Tokenization)를 통해 자연어를 의미 단위로 분할.

### Word Embedding
- 초기 방식: 원-핫 인코딩.
- 발전된 방식: Word2Vec, CBOW 등 알고리즘 사용.

### Encoder-Decoder 구조
- **구조**: 입력을 Context Vector로 변환(Encoder) → Context Vector를 출력으로 변환(Decoder).
- 문제점: Context Vector에 정보 손실 발생.

---

## 3. 어텐션 알고리즘과 트랜스포머
### Attention Mechanism
- **역할**: Decoder가 예측 시점마다 입력 데이터를 다시 참조, 중요한 부분에 집중.
- Query, Key, Value를 통해 입력 데이터 간의 연관성을 계산.

### Transformer
- 2017년 "Attention is All You Need" 논문에서 소개된 모델.
- **특징**:
  - Self-Attention으로 문장 내 모든 단어 관계를 포착.
  - Multi-Head Attention으로 다양한 관계를 학습.
  - RNN 대비 더 빠르고 효율적.

### Transformer 기반 LLM
- 대량의 텍스트 데이터로 학습 가능.
- 자연어 생성뿐만 아니라 다양한 분야에 적용 가능.
- **한계**:
  - 고비용, 데이터 편향 문제, 환각 현상 등.

---

> **과제**: 오늘 배운 내용을 기반으로 LangChain 코드 실습하기.
