## 3기 5주차 자료 요약

---

### Gradient Vanishing 문제
역전파 과정을 거칠 수록 점점 기울기가 0에 가까워짐
- **Sigmoid**: 반복 계산 시 점점 0에 가까운 값을 출력
- **tanh (hyperbolic tangent)**: 개선된 버전이지만 미분 시 0 주위를 제외하면 여전히 0에 수렴
- 여러 개선된 활성화 함수가 있지만 완전한 기울기 소실 문제는 해결되지 않음

### Optimization 기법
- Momentum
    - 이전 단계의 속도를 고려하여 움직임을 조정(관성)

- Nesterov Momentum
    - Momentum의 개선된 버전
    - 현재 위치에서 관성 방향으로 한 번 더 조정하여 세밀한 학습 가능

- Adagrad (Adaptive Gradient Algorithm)
    - 적응형 학습률
    - 기울기 제곱의 누적값에 따라 학습률을 점차 감소
    - 학습이 오래될수록 기울기가 0에 수렴

- RMSProp (Root Mean Square Propagation)
    - Adagrad의 개선책으로, 최근 기울기에 더 큰 가중치를 부여, 오래될 수록 작아짐
    - 학습률이 너무 빠르게 감소하지 않도록 조정

- Adam (Adaptive Moment Estimation)
    - Momentum + RMSProp
    - 각 단계마다 학습률을 자동으로 조정
    - 초기 설정값이 많지만 default으로도 잘 작동

---

### CNN (Convolution Neural Network) 계층 연산
- Convolution 연산을 통해 특성 추출

### Zero Padding
- 경계 정보를 보존하기 위해 이미지에 0을 추가

### Dropout
- 과적합 방지를 위해 일부 뉴런을 랜덤하게 제외
