{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOOE+N6FlPbuchkVRj/mAr6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **시계열 데이터**\n","시간의 흐름에 따라 순차적으로 측정된 데이터\n","\n","\n","---\n","\n"],"metadata":{"id":"FsJP_Me_JHdO"}},{"cell_type":"markdown","source":["# **RNN (Recurrent Neural Network)**\n","\n","임의의 길이를 가진 시퀀스 데이터나 시계열 데이터 분석이 가능한 신경망\n","\n","\n","---\n","- 문장,문서 샘플을 입력으로 받을 수 있고 자연어 처리에 유용\n","\n","---\n","\n","**RNN 단점:**\n","1. 장기 기억 부재\n","2. 학습속도 저하 문제(연산량 많음)\n","3. gradient 소실\n","\n","▶ 단점 극복하기 위한 대표적 모델이 LSTM\n","\n","---\n","\n","1.   **순환 뉴런**\n","- 입력 받아 출력 만들고 자신에게도 출력 보내는 뉴런\n","- 각 타임스텝마다 입력과 이전 타임스텝의 출력을 입력으로 받음\n","2.   **순환 층**\n","- 입출력이 모두 벡터\n","- 입력 위한 가중치와 이전 타임스텝 출력 위한 가중치\n","\n","\n","\n"],"metadata":{"id":"teFHD5_uJ-sX"}},{"cell_type":"markdown","source":["# **LSTM (Long Short Term Memory)**\n","\n","- RNN의 단점 극복하기 위해 만들어진 모델\n","- RNN보다 장기기억력이 뛰어나고 Gradient 소실 문제 개선\n"],"metadata":{"id":"py-AOTGILhwL"}},{"cell_type":"markdown","source":["# **1. Sequnce to Vector Network**\n","- Sequnce를 입력으로 받고 vector를 출력하는 네트워크\n","- 어떤 문장을 입력으로 받고, 감정을 분류하는 모델\n","- 이미지 묘사하는 프롬프트 입력으로 받고 이미지 출력하는 모델\n","\n"],"metadata":{"id":"JPq4CTBYL9_X"}},{"cell_type":"markdown","source":["# **2. Vector to Sequnce Network**\n","- 입력을 vector로 받아 sequnce를 출력하는 모델\n","- 이미지를 입력받아 그에 대한 서술을 출력하는 모델\n","- 단일 단어를 입력해 그에 대한 서술을 출력하는 모델"],"metadata":{"id":"DofMaTozMVdo"}},{"cell_type":"markdown","source":["# **3. Sequnce to Sequnce Network**\n","- 입력과 출력 모두 Sequence인 네트워크\n","- 주식 데이터같은 시계열 데이터 예측에 유용\n","- 일반적인 네트워크의 형태로는 입력과 출력의 사이즈가 달라지는 문제 대응할 수 없음"],"metadata":{"id":"b5uOKyEtM0EB"}},{"cell_type":"markdown","source":["# **자연어 처리**\n","\n","\n","\n","\n","\n","---\n","\n","\n","자연어 제대로 처리하려면 자연어를 숫자로 바꿀 때 의미를 함축하게 만들어야 함. 이 작업 수행 위해 여러 작업 거치지만 완벽할 수는 없음\n","\n"],"metadata":{"id":"ZIjBPe9mNLvU"}},{"cell_type":"markdown","source":["# **Tokenization(토큰화)**\n","\n","정량화 하기 전, 자연어의 경우의 수를 형태소별로 나누기"],"metadata":{"id":"TqOcwyFUNick"}},{"cell_type":"markdown","source":["# **Word Embedding**\n","- 자연어 전처리에서 가장 어렵고 중요한 작업\n","- 각 토큰을 의미가 있는 vector로 변환\n","- 초기에는 원-핫 인코딩 사용\n","- 문장 내 분포, 동시 언급 등을 이용한 알고리즘\n","- 엄청 고차원의 데이터\n"],"metadata":{"id":"C4WqRFSGNpa9"}},{"cell_type":"markdown","source":["# **encoder - decoder**\n","\n","\n","*   정보 손실\n","*   입력이 좀만 길어도 막대한 양의 정보 손실 발생\n","\n"],"metadata":{"id":"7C06z38COnq5"}},{"cell_type":"markdown","source":["# **Attention Mechanibm**\n","- seq2seq의 문제 개선하기 위해 연구된 알고리즘\n","- 전체 입력 문장을 동일한 비율로 참고 x, 해당 시점에서 예측해야 할 단어와 연관 있는 부분을 집중해서 봄\n"],"metadata":{"id":"B5uILBvBPaID"}},{"cell_type":"markdown","source":["# **Transformer**\n","- Attention is all you need에서 처음 공개된 모델 구조\n","- 같은 라파미터 수 대비 RNN보다 빠른 속도\n","- (?) 한 번에 데이터를 입력받음\n","\n","\n","---\n","- 같은 문장 내 모든 단어 쌍 사이의 의미적, 문법적 관계를 포착\n","- Q, K, V가 모두 같은 self attention을 사용\n","- multi - head attention은 최선의 결과를 내기 위해 어텐션 여러번 시행\n","\n","\n","---\n","**응용:**\n","- 토큰을 하나씩 출력해가며 출력한 토큰을 다시 입력하는 과정을 반복해 최종 출력을 만들어내는 것이 생성형 모델 작동 원리\n","\n","\n","\n","---\n","**Transformer 기반 LLM**\n","장점\n","- 이전 언어모델보다 뛰어난 성능, 많은 양의 텍스트로도 가능\n","- 자연어 생성 뿐만 아니라, image, audio 등 다양한 분야에 활용\n","\n","\n","---\n","**Transformer 기반 LLM**  단점\n","\n","- 학습, 튜닝에 비용 ^\n","- 데이터 편향\n","\n","\n"],"metadata":{"id":"KxyyVu40QLir"}},{"cell_type":"markdown","source":["# **벡터의 내적**\n","\n","- 같은 차원끼리 곱한것들끼리의 합\n","- (x1*y1) + (x2*y2) + ... (x4*x4)\n","\n","완전 같은 벡터일 때 내적이 제일 높음.\n","- 벡터가 커지는 방향으로 학습\n","- 31p, animal과 it이 동일어이므로 Attention value가 가장 높음\n"],"metadata":{"id":"drYz4GgnQPIY"}},{"cell_type":"code","source":["유튜브 테디노트?"],"metadata":{"id":"5FIYMoYwbSt6"},"execution_count":null,"outputs":[]}]}